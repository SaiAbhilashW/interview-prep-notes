Kafka Fundamentals

https://sapient.udemy.com/course/apache-kafka/learn/lecture/11566872?start=0#overview

Topic:
------

A particular stream of data
- Similar to a table in a db (without the constraints)
- Identified by its name
- Each topic has one or more partition
- Need to specify the number of partitions in the topic at the time of creation
- 

Partitions:
-----------
Each topic is split into partitions.
Each partition is ordered
Each message in a partition gets an incremental id called offset.

Eg. part 1 - 1 2 3 4 5 6 7
	part 2 - 1 2 3 4
	part 3	- 1 2 3 4 5 6 7 8 9

	- order is guaranteed only within the partition
	- offset 3 in part 1 and part 2 are different data
	- data is erased after some time ( default = 1 week )
	- data written in partition is immutable
	- data is assigned randomly to partitions unless a key is given

2. Broker - Kafka cluster is made of multiple brokers (servers)
		  - Each broker has its unique id (integer)
		  - if connected to one broker, you are connected to the entire cluster
		  - Topic partitions are placed in brokers
		  	Broker1		Broker2		Broker3
		  	T1 p0		T1 p1		T1 p2
		  	T2 p0		T2 p1
		  - Topic should have a replication factor between 2-3
		  - Broker1		Broker2		Broker3
		  	T1 p0		T1 p1		T1 p1
		  				T1 p0

Topic replication

Topic should have a replication factor > 1. Usually 2 or 3.
Suppose Topic A has 2 partitions and it has a replication factor of 2. Then:

Each partition will have 2 replicas.

Broker 1	Broker 2	Broker 3
p0 ta ----> p0 ta		
			p1 ta  ---> p1 ta

So even if we lose Broker 2, Broker 1 and Broker 3 can still serve the data


*****
Leader for a given partition:

At any time there can only be ONE leader broker for a partition which can recieve and serve data. The other brokers will only syncronize and store data.  
So for above made diagram, Broker 1 would be the leader for parttion 1 and Broker 2 would be the ISR (In-Sync Replica)
Zookeeper decides which broker becomes the leader and which becomes the ISR. And if the leader goes down, there is an election and another broker becomes the leader. When the prev leader comes up, it tries to become the leader again


Producers:
----------

Producers write data to topics
Producers know which broker and partition to write
In case of broker failures, producers recover automatically

		/ Broker 1, Topic A, Partition 0
Producer -- Broker 2, Topic A, Partition 1
		\  Broker 3, Topic A, Partition 2

Producer can choose to recieve acknowledgement from the brokers for data writes:
acks=0		=> Producer won't wait for acknowledgement (possible data loss)
acks=1		=> Producer will wait for leader acknowledgement
acks=all 	=> Leader + replica acknowledgement is needed

Producers can choose to send a key with the message which guarantees that the message will go into a particular broker only. This is done using hashing of the key.
If no key is specified, allocated to brokers in a round robin system.


Consumers:
----------

Consumers read data from a topic
consumers know which broker to read from
in case of broker failures, consumers know how to recover
Data within each partition is read 'in order' 

Consumer Groups:
----------------

Consumers read data in consumer Groups
each consumer within a group reads data from exclusive Partitions - no two consumers read from the same partition
If you have more consumers than partitions then some consumers will be inactive

Consumer Offsets:
-----------------

Kafka stores the offset at which the consumer groups have been reading
The offsets are committed live in a kafka topic called __consumer_offsets
When a consumer group has finished processing data recieved from Kafka, it should be committing the offsets
If a consumer dies, it'll be able to read back from where it left off by checking the offset value in the topic

Consumers can choose when to commit the offsets:
3 types:
1. At Most once:
offsets are committed as soon as message is recieved. If processing goes wrong, the message won't be recovered again.
2. At least once:
offests are committed after the message is processed. If processing goes worng, the message can be read again
This can result in duplicate processing of messages. We need to make sure there is no impact even if the message is processed again
3. Exactly once:
Can be achieved for Kafka => Kafka workflows using Streams API. Ideal scenario.

Kafka broker discovery:
-----------------------

Every Kafka broker is also called the bootstrap server. This means you only need to connect to one broker and you will be connected to entire cluster
Each broker knows all about brokers, topics, partitions and metadata

Kafka Client -- connection + metadata request --> any of the bootstrap broker
			
			 <-- List of all the brokers ---

